# RAG Chat System Documentation

## Overview

A **Multi-File RAG (Retrieval-Augmented Generation) Chat System** with **CAG (Cache-Augmented Generation)**, **Multi-Provider LLM Support**, and **Hybrid Search** for optimal performance.

### ğŸŒŸ Key Features

| Feature | Description |
|---------|-------------|
| ğŸ” **Hybrid Search** | Vector similarity + BM25 keyword search combined |
| âš¡ **CAG Caching** | Persistent response cache - instant repeat queries |
| ğŸŒ **Multi-Provider** | OpenAI, Azure, Gemini, Claude, Ollama |
| ğŸ”‘ **Bring Your Own Key** | Use your API keys for cloud providers |
| ğŸ’¬ **Conversation Memory** | Context-aware multi-turn conversations |
| ğŸ“ **Multi-File Upload** | Upload multiple documents at once |
| ğŸ¯ **Reranking** | Cross-encoder reranking for better relevance |
| ğŸ“Š **Progress Tracking** | Real-time progress bar for all operations |
| ğŸ’¾ **Full Persistence** | Survives server/browser restarts |

---

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     STREAMLIT FRONTEND                          â”‚
â”‚  (app.py - Chat UI, File Upload, Provider Selection)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚ HTTP Requests
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     FASTAPI BACKEND                             â”‚
â”‚  (main.py - REST API with CAG Caching)                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  GET  /providers      - List LLM providers                      â”‚
â”‚  GET  /models         - List models for provider                â”‚
â”‚  GET  /documents      - List all documents                      â”‚
â”‚  DELETE /documents    - Delete by index/filename                â”‚
â”‚  DELETE /documents/all- Clear all documents                     â”‚
â”‚  POST /upload         - Upload documents (batch)                â”‚
â”‚  POST /upload_stream  - Upload with progress                    â”‚
â”‚  POST /query          - Query with CAG cache check              â”‚
â”‚  POST /query_stream   - Streaming query                         â”‚
â”‚  POST /refresh        - Reload data from disk                   â”‚
â”‚  GET  /health         - Health check                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼                  â–¼                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   RETRIEVER   â”‚  â”‚   GENERATOR   â”‚  â”‚    CACHE      â”‚
â”‚ FAISS + BM25  â”‚  â”‚ Multi-Providerâ”‚  â”‚  CAG Layer    â”‚
â”‚ + Reranking   â”‚  â”‚   + Streaming â”‚  â”‚  (Persistent) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Project Structure

```
rag_2/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ app.py          # Streamlit frontend
â”‚   â”œâ”€â”€ main.py         # FastAPI backend
â”‚   â”œâ”€â”€ utils.py        # File parsing (PDF, DOCX, CSV, TXT)
â”‚   â”œâ”€â”€ chunker.py      # Sentence-based chunking
â”‚   â”œâ”€â”€ embeddings.py   # Ollama embeddings with caching
â”‚   â”œâ”€â”€ retriever.py    # Hybrid search + reranking
â”‚   â”œâ”€â”€ generator.py    # Multi-provider LLM generation
â”‚   â””â”€â”€ cache.py        # CAG persistent caching
â”œâ”€â”€ data/               # Persistent storage (auto-created)
â”‚   â”œâ”€â”€ documents.json
â”‚   â”œâ”€â”€ document_metadata.json
â”‚   â”œâ”€â”€ faiss_index.bin
â”‚   â”œâ”€â”€ embedding_cache.pkl
â”‚   â”œâ”€â”€ query_cache.json
â”‚   â”œâ”€â”€ response_cache.json   # Full response CAG cache
â”‚   â”œâ”€â”€ chat_history.json
â”‚   â””â”€â”€ api_keys.json
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ run_optimized.py    # Start both servers
â””â”€â”€ DOCUMENTATION.md
```

---

## ğŸŒ Multi-Provider LLM Support

### Supported Providers

| Provider | Models | Requirements |
|----------|--------|--------------|
| **Ollama** (Local) | gpt-oss, llama3.2, mistral, qwen3 | Local install |
| **OpenAI** | gpt-4o, gpt-4o-mini, gpt-3.5-turbo | API key |
| **Azure OpenAI** | gpt-4, gpt-4o, gpt-35-turbo | API key + Endpoint |
| **Google Gemini** | gemini-1.5-pro, gemini-1.5-flash | API key |
| **Anthropic Claude** | claude-3-5-sonnet, claude-3-5-haiku | API key |

### Getting API Keys

1. **OpenAI**: https://platform.openai.com/api-keys
2. **Azure**: Azure Portal â†’ OpenAI resource â†’ Keys and Endpoint
3. **Gemini**: https://makersuite.google.com/app/apikey
4. **Claude**: https://console.anthropic.com/

### Usage
1. Select provider from sidebar dropdown
2. Enter API key in the secure input field
3. Select model for that provider
4. Start chatting!

---

## âš¡ CAG (Cache-Augmented Generation)

### What is CAG?

CAG caches **complete responses** (not just embeddings) to disk, enabling instant responses for repeated queries - even across server restarts!

### Cache Layers

| Layer | File | TTL | Purpose |
|-------|------|-----|---------|
| **Embedding Cache** | `embedding_cache.pkl` | Permanent | Skip re-embedding same text |
| **Query Cache** | `query_cache.json` | 1 hour | Skip re-retrieval for same query |
| **Response Cache** | `response_cache.json` | 1 hour | Skip LLM call entirely! |

### How It Works

```
User asks question
    â†“
Check Response Cache â†’ HIT? â†’ Return instant answer âš¡
    â†“ MISS
Retrieve documents (check Query Cache)
    â†“
Generate answer with LLM
    â†“
Cache full response to disk
    â†“
Return answer
```

### Cache Invalidation

Caches are automatically cleared when:
- Documents are uploaded
- Documents are deleted
- User clicks "Clear All Data"

---

## ğŸ” Hybrid Search

The system uses **Hybrid Search** combining:

### Vector Search (60% weight)
- FAISS IndexFlatL2 for semantic similarity
- 768-dimensional embeddings (nomic-embed-text)
- Finds conceptually similar content

### Keyword Search (40% weight)  
- BM25 algorithm for exact term matching
- Handles specific names, codes, acronyms
- Complements semantic search

### Reranking (Optional)
When enabled, results are re-scored using:
1. **Cross-Encoder** (ms-marco-MiniLM-L-6-v2) - More accurate
2. **Local Scoring** (fallback) - Faster, based on term overlap

---

## ğŸ“Š Progress Tracking

Real-time progress bar shows all stages:

| Stage | Progress | Status |
|-------|----------|--------|
| ğŸ” | 0-10% | Checking CAG cache |
| âš¡ | 100% | CAG CACHE HIT (instant!) |
| ğŸ“¡ | 20% | Connecting to server |
| ğŸ” | 30% | Searching documents |
| ğŸ“š | 50% | Found X relevant chunks |
| ğŸ¤– | 60-70% | Generating response |
| âœ… | 100% | Complete |

---

## ğŸ’¬ Conversation Memory

The system maintains conversation context:

- Last 6 messages included in LLM prompt
- Enables follow-up questions like "Tell me more" or "What about X?"
- Each chat session has independent history
- History persists across sessions

---

## ğŸ“ Document Management

### Supported Formats
- `.txt` - Plain text
- `.pdf` - PDF documents
- `.docx` - Word documents  
- `.csv` - CSV files

### Multi-File Upload
- Upload multiple files at once
- Progress bar per file
- Automatic chunking and embedding

### Chunking Strategy
- Sentence-based splitting (NLTK)
- Max 2000 characters per chunk
- 2 sentence overlap for context continuity

---

## ğŸš€ Deployment to Streamlit Cloud

### Prerequisites
1. GitHub account
2. Streamlit Cloud account (https://streamlit.io/cloud)
3. Repository with your code

### Setup Steps

#### 1. Create `requirements.txt`
```
streamlit
fastapi
uvicorn
python-multipart
requests
PyPDF2
python-docx
pandas
nltk
faiss-cpu
numpy
ollama
sentence-transformers
rank-bm25
```

#### 2. Create `.streamlit/secrets.toml` (for API keys)
```toml
[api_keys]
openai = "sk-..."
azure_key = "..."
azure_endpoint = "https://..."
gemini = "..."
claude = "..."
```

#### 3. Create `packages.txt` (for system dependencies)
```
build-essential
```

#### 4. Modify for Cloud Deployment

**Important**: Streamlit Cloud runs only the Streamlit app. You need to either:

**Option A: Embed FastAPI in Streamlit (Recommended)**
- Run FastAPI in a background thread
- Use `threading` module

**Option B: Use External API**
- Deploy FastAPI separately (Railway, Render, etc.)
- Update `API_BASE` URL in app.py

### Cloud Limitations
- No local Ollama (use cloud providers instead)
- File storage is temporary (use cloud storage for persistence)
- Memory limits apply

---

## Running Locally

### 1. Install Dependencies
```bash
pip install -r requirements.txt
python -c "import nltk; nltk.download('punkt')"
```

### 2. Start Ollama (for local LLM)
```bash
ollama pull nomic-embed-text
ollama pull gpt-oss
ollama serve
```

### 3. Start the Application
```bash
# Option 1: Use the launcher script
python run_optimized.py

# Option 2: Start manually
# Terminal 1:
uvicorn app.main:app --reload --port 8000

# Terminal 2:
streamlit run app/app.py
```

### 4. Access
- Streamlit UI: http://localhost:8501
- FastAPI docs: http://localhost:8000/docs

---

## API Reference

### Providers & Models
```http
GET /providers
Response: {"providers": {"ollama": {...}, "openai": {...}, ...}}

GET /models?provider=openai
Response: {"models": ["gpt-4o", "gpt-4o-mini", ...]}
```

### Documents
```http
GET /documents
Response: {"count": 10, "documents": [{...}, ...]}

DELETE /documents
Body: {"filename": "doc.pdf"}
Response: {"status": "deleted", "chunks_deleted": 5}

DELETE /documents/all
Response: {"status": "cleared"}

POST /refresh
Response: {"status": "refreshed", "document_count": 10}
```

### Upload
```http
POST /upload
Body: multipart/form-data (file)
Response: {"status": "success", "chunks_added": 15}

POST /upload_stream
Body: multipart/form-data (file)
Response: Server-Sent Events
```

### Query
```http
POST /query
Body: {
  "query": "What is...",
  "model": "gpt-4o",
  "provider": "openai",
  "api_key": "sk-...",
  "search_mode": "hybrid",
  "use_reranking": true,
  "top_k": 3
}
Response: {"answer": "...", "retrieved_docs": [...], "cached": true/false}

POST /query_stream
Response: Server-Sent Events with tokens
```

---

## Troubleshooting

| Issue | Solution |
|-------|----------|
| "I don't know" responses | Check documents are uploaded, try refreshing |
| Slow first query | Normal - embeddings being generated |
| Instant second query | CAG cache working! âœ“ |
| Connection error | Ensure FastAPI is running on port 8000 |
| Provider error | Check API key is valid |
| Memory error | Reduce chunk size or use smaller model |

---

## Performance Tips

1. **Use CAG**: Repeat queries are instant
2. **Enable Reranking**: Better relevance at slight speed cost
3. **Optimal Chunk Size**: 2000 chars balances context vs precision
4. **Hybrid Search**: Always enabled for best results
5. **Cloud Providers**: Faster than local Ollama for generation

---

## Security Notes

- API keys stored locally in `data/api_keys.json`
- Keys passed per-request, not stored on server
- For production: use environment variables or secrets manager
- Clear browser data to remove stored keys

---

## Future Improvements

- [ ] Multi-user authentication
- [ ] Cloud storage integration (S3, GCS)
- [ ] Webhook notifications
- [ ] Advanced analytics dashboard
- [ ] Custom embedding models
- [ ] PDF page citations

---

*Documentation updated: January 1, 2026*
